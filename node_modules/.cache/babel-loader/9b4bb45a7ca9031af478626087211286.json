{"ast":null,"code":"import _regeneratorRuntime from \"C:\\\\Users\\\\adity\\\\OneDrive\\\\Desktop\\\\react-sentiment-analyzer\\\\node_modules\\\\@babel\\\\runtime/regenerator\";\nimport _asyncToGenerator from \"C:\\\\Users\\\\adity\\\\OneDrive\\\\Desktop\\\\react-sentiment-analyzer\\\\node_modules\\\\@babel\\\\runtime/helpers/esm/asyncToGenerator\";\nimport * as toxicity from '@tensorflow-models/toxicity';\nvar THRESHOLD = 0.9;\nexport default function toxicityAnalyzer(_x) {\n  return _toxicityAnalyzer.apply(this, arguments);\n}\n\nfunction _toxicityAnalyzer() {\n  _toxicityAnalyzer = _asyncToGenerator(\n  /*#__PURE__*/\n  _regeneratorRuntime.mark(function _callee(text) {\n    var model;\n    return _regeneratorRuntime.wrap(function _callee$(_context) {\n      while (1) {\n        switch (_context.prev = _context.next) {\n          case 0:\n            _context.next = 2;\n            return toxicity.load(THRESHOLD);\n\n          case 2:\n            model = _context.sent;\n            _context.next = 5;\n            return model.classify(text);\n\n          case 5:\n            return _context.abrupt(\"return\", _context.sent);\n\n          case 6:\n          case \"end\":\n            return _context.stop();\n        }\n      }\n    }, _callee);\n  }));\n  return _toxicityAnalyzer.apply(this, arguments);\n}","map":{"version":3,"sources":["C:\\Users\\adity\\OneDrive\\Desktop\\react-sentiment-analyzer\\src\\utils\\toxicityAnalyzer.js"],"names":["toxicity","THRESHOLD","toxicityAnalyzer","text","load","model","classify"],"mappings":";;AAAA,OAAO,KAAKA,QAAZ,MAA0B,6BAA1B;AAEA,IAAMC,SAAS,GAAG,GAAlB;AAEA,wBAA8BC,gBAA9B;AAAA;AAAA;;;;;2BAAe,iBAAgCC,IAAhC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mBACSH,QAAQ,CAACI,IAAT,CAAcH,SAAd,CADT;;AAAA;AACLI,YAAAA,KADK;AAAA;AAAA,mBAEEA,KAAK,CAACC,QAAN,CAAeH,IAAf,CAFF;;AAAA;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,G","sourcesContent":["import * as toxicity from '@tensorflow-models/toxicity';\r\n\r\nconst THRESHOLD = 0.9;\r\n\r\nexport default async function toxicityAnalyzer(text) {\r\n    const model = await toxicity.load(THRESHOLD);\r\n    return await model.classify(text);\r\n}\r\n"]},"metadata":{},"sourceType":"module"}