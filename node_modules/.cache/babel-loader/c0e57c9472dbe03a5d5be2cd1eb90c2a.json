{"ast":null,"code":"//     wink-tokenizer\n//     Multilingual tokenizer that automatically tags each token with its type.\n//\n//     Copyright (C) 2017-19  GRAYPE Systems Private Limited\n//\n//     This file is part of ‚Äúwink-tokenizer‚Äù.\n//\n//     Permission is hereby granted, free of charge, to any person obtaining a\n//     copy of this software and associated documentation files (the \"Software\"),\n//     to deal in the Software without restriction, including without limitation\n//     the rights to use, copy, modify, merge, publish, distribute, sublicense,\n//     and/or sell copies of the Software, and to permit persons to whom the\n//     Software is furnished to do so, subject to the following conditions:\n//\n//     The above copyright notice and this permission notice shall be included\n//     in all copies or substantial portions of the Software.\n//\n//     THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n//     OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n//     FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n//     THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n//     LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n//     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n//     DEALINGS IN THE SOFTWARE.\n//\nvar contractions = require('./eng-contractions.js');\n\nvar rgxSpaces = /\\s+/g; // Ordinals only for Latin like 1st, 2nd or 12th or 33rd.\n\nvar rgxOrdinalL1 = /1\\dth|[04-9]th|1st|2nd|3rd|[02-9]1st|[02-9]2nd|[02-9]3rd|[02-9][04-9]th|\\d+\\d[04-9]th|\\d+\\d1st|\\d+\\d2nd|\\d+\\d3rd/g; // Apart from detecting pure integers or decimals, also detect numbers containing\n// `. - / ,` so that dates, ip address, fractions and things like codes or part\n// numbers are also detected as numbers only. These regex will therefore detected\n// 8.8.8.8 or 12-12-1924 or 1,1,1,1.00 or 1/4 or 1/4/66/777 as numbers.\n// Latin-1 Numbers.\n\nvar rgxNumberL1 = /\\d+\\/\\d+|\\d(?:[\\.,-\\/]?\\d)*(?:\\.\\d+)?/g; // Devanagari Numbers.\n\nvar rgxNumberDV = /[\\u0966-\\u096F]+\\/[\\u0966-\\u096F]+|[\\u0966-\\u096F](?:[\\.,-\\/]?[\\u0966-\\u096F])*(?:\\.[\\u0966-\\u096F]+)?/g;\nvar rgxMention = /@\\w+/g; // Latin-1 Hashtags.\n\nvar rgxHashtagL1 = /#[a-z][a-z0-9]*/gi; // Devanagari Hashtags; include Latin-1 as well.\n\nvar rgxHashtagDV = /#[\\u0900-\\u0963\\u0970-\\u097F][\\u0900-\\u0963\\u0970-\\u097F\\u0966-\\u096F0-9]*/gi; // EMail is EN character set.\n\nvar rgxEmail = /[-!#$%&'*+\\/=?^\\w{|}~](?:\\.?[-!#$%&'*+\\/=?^\\w`{|}~])*@[a-z0-9](?:-?\\.?[a-z0-9])*(?:\\.[a-z](?:-?[a-z0-9])*)+/gi; // Bitcoin, Ruble, Indian Rupee, Other Rupee, Dollar, Pound, Yen, Euro, Wong.\n\nvar rgxCurrency = /[‚Çø‚ÇΩ‚Çπ‚Ç®$¬£¬•‚Ç¨‚Ç©]/g; // These include both the punctuations: Latin-1 & Devanagari.\n\nvar rgxPunctuation = /[‚Äô'‚Äò‚Äô`‚Äú‚Äù\"\\[\\]\\(\\){}‚Ä¶,\\.!;\\?\\-:\\u0964\\u0965]/g;\nvar rgxQuotedPhrase = /\"[^\"]*\"/g; // NOTE: URL will support only EN character set for now.\n\nvar rgxURL = /(?:https?:\\/\\/)(?:[\\da-z\\.-]+)\\.(?:[a-z\\.]{2,6})(?:[\\/\\w\\.\\-\\?#=]*)*\\/?/gi;\nvar rgxEmoji = /[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]|[\\u2600-\\u26FF]|[\\u2700-\\u27BF]/g;\nvar rgxEmoticon = /:-?[dps\\*\\/\\[\\]{}\\(\\)]|;-?[/(/)d]|<3/gi;\nvar rgxTime = /(?:\\d|[01]\\d|2[0-3]):?(?:[0-5][0-9])?\\s?(?:[ap]\\.?m\\.?|hours|hrs)/gi; // Inlcude [Latin-1 Supplement Unicode Block](https://en.wikipedia.org/wiki/Latin-1_Supplement_(Unicode_block))\n\nvar rgxWordL1 = /[a-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u00FF][a-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u00FF']*/gi; // Define [Devanagari Unicode Block](https://unicode.org/charts/PDF/U0900.pdf)\n\nvar rgxWordDV = /[\\u0900-\\u094F\\u0951-\\u0963\\u0970-\\u097F]+/gi; // Symbols go here; including Om.\n\nvar rgxSymbol = /[\\u0950~@#%\\^\\+=\\*\\|\\/<>&]/g; // For detecting if the word is a potential contraction.\n\nvar rgxContraction = /'/; // Singular & Plural possessive\n\nvar rgxPosSingular = /([a-z]+)('s)$/i;\nvar rgxPosPlural = /([a-z]+s)(')$/i; // Regexes and their categories; used for tokenizing via match/split. The\n// sequence is *critical* for correct tokenization.\n\nvar rgxsMaster = [{\n  regex: rgxQuotedPhrase,\n  category: 'quoted_phrase'\n}, {\n  regex: rgxURL,\n  category: 'url'\n}, {\n  regex: rgxEmail,\n  category: 'email'\n}, {\n  regex: rgxMention,\n  category: 'mention'\n}, {\n  regex: rgxHashtagL1,\n  category: 'hashtag'\n}, {\n  regex: rgxHashtagDV,\n  category: 'hashtag'\n}, {\n  regex: rgxEmoji,\n  category: 'emoji'\n}, {\n  regex: rgxEmoticon,\n  category: 'emoticon'\n}, {\n  regex: rgxTime,\n  category: 'time'\n}, {\n  regex: rgxOrdinalL1,\n  category: 'ordinal'\n}, {\n  regex: rgxNumberL1,\n  category: 'number'\n}, {\n  regex: rgxNumberDV,\n  category: 'number'\n}, {\n  regex: rgxCurrency,\n  category: 'currency'\n}, {\n  regex: rgxWordL1,\n  category: 'word'\n}, {\n  regex: rgxWordDV,\n  category: 'word'\n}, {\n  regex: rgxPunctuation,\n  category: 'punctuation'\n}, {\n  regex: rgxSymbol,\n  category: 'symbol'\n}]; // Used to generate finger print from the tokens.\n// NOTE: this variable is being reset in `defineConfig()`.\n\nvar fingerPrintCodes = {\n  emoticon: 'c',\n  email: 'e',\n  emoji: 'j',\n  hashtag: 'h',\n  mention: 'm',\n  number: 'n',\n  ordinal: 'o',\n  quoted_phrase: 'q',\n  // eslint-disable-line camelcase\n  currency: 'r',\n  // symbol: 's',\n  time: 't',\n  url: 'u',\n  word: 'w',\n  alien: 'z'\n}; // ### tokenizer\n\n/**\n *\n * Creates an instance of {@link Tokenizer}.\n *\n * @return {Tokenizer} object conatining set of API methods for tokenizing a sentence\n * and defining configuration, plugin etc.\n * @example\n * // Load wink tokenizer.\n * var tokenizer = require( 'wink-tokenizer' );\n * // Create your instance of wink tokenizer.\n * var myTokenizer = tokenizer();\n*/\n\nvar tokenizer = function tokenizer() {\n  // Default configuration: most comprehensive tokenization. Make deep copy!\n  var rgxs = rgxsMaster.slice(0); // The result of last call to `tokenize()` is retained here.\n\n  var finalTokens = []; // Returned!\n\n  /**\n   * @classdesc Tokenizer class\n   * @class Tokenizer\n   * @hideconstructor\n   */\n\n  var methods = Object.create(null); // ### manageContraction\n\n  /**\n   *\n   * Splits a contractions into words by first trying a lookup in strandard\n   * `contractions`; if the lookup fails, it checks for possessive in `'s` or\n   * `s'` forms and separates the possesive part from the word. Otherwise the\n   * contraction is treated as a normal word and no splitting occurs.\n   *\n   * @param {string} word that could be a potential conraction.\n   * @param {object[]} tokens where the outcome is pushed.\n   * @return {object[]} updated tokens according to the `word.`\n   * @private\n  */\n\n  var manageContraction = function manageContraction(word, tokens) {\n    var ct = contractions[word];\n    var matches;\n\n    if (ct === undefined) {\n      // Try possesive of sigular & plural forms\n      matches = word.match(rgxPosSingular);\n\n      if (matches) {\n        tokens.push({\n          value: matches[1],\n          tag: 'word'\n        });\n        tokens.push({\n          value: matches[2],\n          tag: 'word'\n        });\n      } else {\n        matches = word.match(rgxPosPlural);\n\n        if (matches) {\n          tokens.push({\n            value: matches[1],\n            tag: 'word'\n          });\n          tokens.push({\n            value: matches[2],\n            tag: 'word'\n          });\n        } else tokens.push({\n          value: word,\n          tag: 'word'\n        });\n      }\n    } else {\n      // Manage via lookup; ensure cloning!\n      tokens.push(Object.assign({}, ct[0]));\n      tokens.push(Object.assign({}, ct[1]));\n      if (ct[2]) tokens.push(Object.assign({}, ct[2]));\n    }\n\n    return tokens;\n  }; // manageContraction()\n  // ### tokenizeTextUnit\n\n  /**\n   *\n   * Attempts to tokenize the input `text` using the `rgxSplit`. The tokenization\n   * is carried out by combining the regex matches and splits in the right sequence.\n   * The matches are the *real tokens*, whereas splits are text units that are\n   * tokenized in later rounds! The real tokens (i.e. matches) are pushed as\n   * `object` and splits as `string`.\n   *\n   * @param {string} text unit that is to be tokenized.\n   * @param {object} rgxSplit object containing the regex and it's category.\n   * @return {array} of tokens.\n   * @private\n  */\n\n\n  var tokenizeTextUnit = function tokenizeTextUnit(text, rgxSplit) {\n    // Regex matches go here; note each match is a token and has the same tag\n    // as of regex's category.\n    var matches = text.match(rgxSplit.regex); // Balance is \"what needs to be tokenized\".\n\n    var balance = text.split(rgxSplit.regex); // The result, in form of combination of tokens & matches, is captured here.\n\n    var tokens = []; // The tag;\n\n    var tag = rgxSplit.category; // Helper variables.\n\n    var aword,\n        i,\n        imax,\n        k = 0,\n        t; // Combine tokens & matches in the following pattern [ b0 m0 b1 m1 ... ]\n\n    matches = matches ? matches : [];\n\n    for (i = 0, imax = balance.length; i < imax; i += 1) {\n      t = balance[i];\n      t = t.trim();\n      if (t) tokens.push(t);\n\n      if (k < matches.length) {\n        if (tag === 'word') {\n          // Tag type `word` token may have a contraction.\n          aword = matches[k];\n\n          if (rgxContraction.test(aword)) {\n            tokens = manageContraction(aword, tokens);\n          } else {\n            // Means there is no contraction.\n            tokens.push({\n              value: aword,\n              tag: tag\n            });\n          }\n        } else tokens.push({\n          value: matches[k],\n          tag: tag\n        });\n      }\n\n      k += 1;\n    }\n\n    return tokens;\n  }; // tokenizeTextUnit()\n  // ### tokenizeTextRecursively\n\n  /**\n   *\n   * Tokenizes the input text recursively using the array of `regexes` and then\n   * the `tokenizeTextUnit()` function. If (or whenever) the `regexes` becomes\n   * empty, it simply splits the text on non-word characters instead of using\n   * the `tokenizeTextUnit()` function.\n   *\n   * @param {string} text unit that is to be tokenized.\n   * @param {object} regexes object containing the regex and it's category.\n   * @return {undefined} nothing!\n   * @private\n  */\n\n\n  var tokenizeTextRecursively = function tokenizeTextRecursively(text, regexes) {\n    var sentence = text.trim();\n    var tokens = [];\n    var i, imax;\n\n    if (!regexes.length) {\n      // No regex left, split on `spaces` and tag every token as **alien**.\n      text.split(rgxSpaces).forEach(function (tkn) {\n        finalTokens.push({\n          value: tkn.trim(),\n          tag: 'alien'\n        });\n      });\n      return;\n    }\n\n    var rgx = regexes[0];\n    tokens = tokenizeTextUnit(sentence, rgx);\n\n    for (i = 0, imax = tokens.length; i < imax; i += 1) {\n      if (typeof tokens[i] === 'string') {\n        // Strings become candidates for further tokenization.\n        tokenizeTextRecursively(tokens[i], regexes.slice(1));\n      } else {\n        finalTokens.push(tokens[i]);\n      }\n    }\n  }; // tokenizeTextRecursively()\n  // ### defineConfig\n\n  /**\n   *\n   * Defines the configuration in terms of the types of token that will be\n   * extracted by [`tokenize()`](#tokenize) method. Note by default, all types\n   * of tokens will be detected and tagged automatically.\n   *\n   * @method Tokenizer#defineConfig\n   * @param {object} config It defines 0 or more properties from the list of\n   * **14** properties. A true value for a property ensures tokenization\n   * for that type of text; whereas false value will mean that the tokenization of that\n   * type of text will not be attempted. It also **resets** the effect of any previous\n   * call(s) to the [`addRegex()`](#addregex) API.\n   *\n   * *An empty config object is equivalent to splitting on spaces. Whatever tokens\n   * are created like this are tagged as **alien** and **`z`** is the\n   * [finger print](#gettokensfp) code of this token type.*\n   *\n   * The table below gives the name of each property and it's description including\n   * examples. The character with in paranthesis is the [finger print](#gettokensfp) code for the\n   * token of that type.\n   * @param {boolean} [config.currency=true] such as **$** or **¬£** symbols (**`r`**)\n   * @param {boolean} [config.email=true] for example **john@acme.com** or **superman1@gmail.com** (**`e`**)\n   * @param {boolean} [config.emoji=true] any standard unicode emojis e.g. üòä or üòÇ or üéâ (**`j`**)\n   * @param {boolean} [config.emoticon=true] common emoticons such as **`:-)`** or **`:D`** (**`c`**)\n   * @param {boolean} [config.hashtag=true] hash tags such as **`#happy`** or **`#followme`** (**`h`**)\n   * @param {boolean} [config.number=true] any integer, decimal number, fractions such as **19**, **2.718**\n   * or **1/4** and numerals containing \"**`, - / .`**\", for example 12-12-1924 (**`n`**)\n   * @param {boolean} [config.ordinal=true] ordinals like **1st**, **2nd**, **3rd**, **4th** or **12th** or **91st** (**`o`**)\n   * @param {boolean} [config.punctuation=true] common punctuation such as **`?`** or **`,`**\n   * ( token becomes fingerprint )\n   * @param {boolean} [config.quoted_phrase=false] any **\"quoted text\"** in the sentence. _Note: its default value is **false**._ (**`q`**)\n   * @param {boolean} [config.symbol=true] for example **`~`** or **`+`** or **`&`** or **`%`** or **`/`** ( token becomes fingerprint )\n   * @param {boolean} [config.time=true] common representation of time such as **4pm** or **16:00 hours** (**`t`**)\n   * @param {boolean} [config.mention=true] **@mention**  as in github or twitter (**`m`**)\n   * @param {boolean} [config.url=true] URL such as **https://github.com** (**`u`**)\n   * @param {boolean} [config.word=true] word such as **faster** or **r√©sum√©** or **pr√©venir** (**`w`**)\n   * @return {number} number of properties set to true from the list of above 13.\n   * @example\n   * // Do not tokenize & tag @mentions.\n   * var myTokenizer.defineConfig( { mention: false } );\n   * // -> 13\n   * // Only tokenize words as defined above.\n   * var myTokenizer.defineConfig( {} );\n   * // -> 0\n  */\n\n\n  var defineConfig = function defineConfig(config) {\n    if (typeof config === 'object' && Object.keys(config).length) {\n      rgxs = rgxsMaster.filter(function (rgx) {\n        // Config for the Category of `rgx`.\n        var cc = config[rgx.category]; // Means `undefined` & `null` values are taken as true; otherwise\n        // standard **truthy** and **falsy** interpretation applies!!\n\n        return cc === undefined || cc === null || !!cc;\n      });\n    } else rgxs = []; // Count normalized length i.e. ignore multi-script entries.\n\n\n    var uniqueCats = Object.create(null);\n    rgxs.forEach(function (rgx) {\n      uniqueCats[rgx.category] = true;\n    }); // Reset the `fingerPrintCodes` variable.\n\n    fingerPrintCodes = {\n      emoticon: 'c',\n      email: 'e',\n      emoji: 'j',\n      hashtag: 'h',\n      mention: 'm',\n      number: 'n',\n      ordinal: 'o',\n      quoted_phrase: 'q',\n      // eslint-disable-line camelcase\n      currency: 'r',\n      // symbol: 's',\n      time: 't',\n      url: 'u',\n      word: 'w',\n      alien: 'z'\n    };\n    return Object.keys(uniqueCats).length;\n  }; // defineConfig()\n  // ### tokenize\n\n  /**\n   *\n   * Tokenizes the input `sentence` using the configuration specified via\n   * [`defineConfig()`](#defineconfig).\n   * Common contractions and possessive nouns are split into 2 separate tokens;\n   * for example **I'll** splits as `'I'` and `'\\'ll'` or **won't** splits as\n   * `'wo'` and `'n\\'t'`.\n   *\n   * @method Tokenizer#tokenize\n   * @param {string} sentence the input sentence.\n   * @return {object[]} of tokens; each one of them is an object with 2-keys viz.\n   * `value` and its `tag` identifying the type of the token.\n   * @example\n   * var s = 'For detailed API docs, check out http://winkjs.org/wink-regression-tree/ URL!';\n   * myTokenizer.tokenize( s );\n   * // -> [ { value: 'For', tag: 'word' },\n   * //      { value: 'detailed', tag: 'word' },\n   * //      { value: 'API', tag: 'word' },\n   * //      { value: 'docs', tag: 'word' },\n   * //      { value: ',', tag: 'punctuation' },\n   * //      { value: 'check', tag: 'word' },\n   * //      { value: 'out', tag: 'word' },\n   * //      { value: 'http://winkjs.org/wink-regression-tree/', tag: 'url' },\n   * //      { value: 'URL', tag: 'word' },\n   * //      { value: '!', tag: 'punctuation' } ]\n  */\n\n\n  var tokenize = function tokenize(sentence) {\n    finalTokens = [];\n    tokenizeTextRecursively(sentence, rgxs);\n    return finalTokens;\n  }; // tokenize()\n  // ### getTokensFP\n\n  /**\n   *\n   * Returns the finger print of the tokens generated by the last call to\n   * [`tokenize()`](#tokenize). A finger print is a string created by sequentially\n   * joining the unique code of each token's type. Refer to table given under\n   * [`defineConfig()`](#defineconfig) for values of these codes.\n   *\n   * A finger print is extremely useful in spotting patterns present in the sentence\n   * using `regexes`, which is otherwise a complex and time consuming task.\n   *\n   * @method Tokenizer#getTokensFP\n   * @return {string} finger print of tokens generated by the last call to `tokenize()`.\n   * @example\n   * // Generate finger print of sentence given in the previous example\n   * // under tokenize().\n   * myTokenizer.getTokensFP();\n   * // -> 'wwww,wwuw!'\n  */\n\n\n  var getTokensFP = function getTokensFP() {\n    var fp = [];\n    finalTokens.forEach(function (t) {\n      fp.push(fingerPrintCodes[t.tag] ? fingerPrintCodes[t.tag] : t.value);\n    });\n    return fp.join('');\n  }; // getFingerprint()\n  // ### addTag\n\n\n  var addTag = function addTag(name, fingerprintCode) {\n    if (fingerPrintCodes[name]) {\n      throw new Error('Tag ' + name + ' already exists');\n    }\n\n    fingerPrintCodes[name] = fingerprintCode;\n  }; // addTag()\n  // ### addRegex\n\n  /**\n   * Adds a regex for parsing a new type of token. This regex can either be mapped\n   * to an existing tag or it allows creation of a new tag along with its finger print.\n   * The uniqueness of the [finger prints](#defineconfig) have to ensured by the user.\n   *\n   * *The added regex(s) will supersede the internal parsing.*\n   *\n   * @method Tokenizer#addRegex\n   * @param {RegExp} regex the new regular expression.\n   * @param {string} tag tokens matching the `regex` will be assigned this tag.\n   * @param {string} [fingerprintCode=undefined] required if adding a new\n   * tag; ignored if using an existing tag.\n   * @return {void} nothing!\n   * @example\n   * // Adding a regex for an existing tag\n   * myTokenizer.addRegex( /\\(oo\\)/gi, 'emoticon' );\n   * myTokenizer.tokenize( '(oo) Hi!' )\n   * // -> [ { value: '(oo)', tag: 'emoticon' },\n   * //      { value: 'Hi', tag: 'word' },\n   * //      { value: '!', tag: 'punctuation' } ]\n   *\n   * // Adding a regex to parse a new token type\n   * myTokenizer.addRegex( /hello/gi, 'greeting', 'g' );\n   * myTokenizer.tokenize( 'hello, how are you?' );\n   * // -> [ { value: 'hello', tag: 'greeting' },\n   * //      { value: ',', tag: 'punctuation' },\n   * //      { value: 'how', tag: 'word' },\n   * //      { value: 'are', tag: 'word' },\n   * //      { value: 'you', tag: 'word' },\n   * //      { value: '?', tag: 'punctuation' } ]\n   * // Notice how \"hello\" is now tagged as \"greeting\" and not as \"word\".\n   *\n   * // Using definConfig will reset the above!\n   * myTokenizer.defineConfig( { word: true } );\n   * myTokenizer.tokenize( 'hello, how are you?' );\n   * // -> [ { value: 'hello', tag: 'word' },\n   * //      { value: ',', tag: 'punctuation' },\n   * //      { value: 'how', tag: 'word' },\n   * //      { value: 'are', tag: 'word' },\n   * //      { value: 'you', tag: 'word' },\n   * //      { value: '?', tag: 'punctuation' } ]\n  */\n\n\n  var addRegex = function addRegex(regex, tag, fingerprintCode) {\n    if (!fingerPrintCodes[tag] && !fingerprintCode) {\n      throw new Error('Tag ' + tag + ' doesn\\'t exist; Provide a \\'fingerprintCode\\' to add it as a tag.');\n    } else if (!fingerPrintCodes[tag]) {\n      addTag(tag, fingerprintCode);\n    }\n\n    rgxs.unshift({\n      regex: regex,\n      category: tag\n    });\n  }; // addRegex()\n  // Set quoted_phrase as false becuase mostly it is not required.\n\n\n  defineConfig({\n    quoted_phrase: false\n  }); // eslint-disable-line camelcase\n\n  methods.defineConfig = defineConfig;\n  methods.tokenize = tokenize;\n  methods.getTokensFP = getTokensFP;\n  methods.addTag = addTag;\n  methods.addRegex = addRegex;\n  return methods;\n};\n\nmodule.exports = tokenizer;","map":null,"metadata":{},"sourceType":"script"}